
# EqualEyes
This project creates a new way for blind people to enjoy movies and videos on laptops. It uses clever computer programs to understand what's happening on the screen and then describes it out loud in a way that goes along with the movie's sound. This narration fills in the gaps for viewers who can't see, By doing this, the project aims to give blind people a richer entertainment experience and let them enjoy the power of movies just like everyone else.
## how this work

Our prototype, designed to cater to the accessibility needs of blind individuals, revolutionizes the video consumption experience by providing detailed audio descriptions synchronized seamlessly with visual content. Utilizing cutting-edge object detection models and pose estimation algorithms, our system accurately analyzes the pose of individuals and identifies various objects in real-time with millisecond precision. The dynamic feature incorporated in our platform delivers concise explanations of detected entities at 10-second intervals, ensuring enhanced user understanding and engagement. By continuously updating our platform, we are committed to refining the user experience and ensuring inclusivity for deaf-blind individuals, thus bridging the accessibility gap in video content consumption.
## Tech Stack

**language:** python

**ML algorithm :** YOLO V8 

**library used:** opencv, pyttsx3




## coming soon updates 
- will add pose estimations algorithm (YOLOv8 Pose Estimation)

- NLP models can be trained to understand individual preferences and tailor the generated descriptions accordingly
## Acknowledgements

 - [About yolo V8](https://docs.ultralytics.com/)
 - [Pose Estimation](https://docs.ultralytics.com/tasks/pose/)
 
## License

[MIT](https://github.com/navajyoth-puthalath13/EqualEyes/commit/43718f815cd7b0ba1129859f31354b525e893bf8)


## Documentation

[Documentation](https://drive.google.com/file/d/1Y6YJWiWQhcB-MngmEviTZfuQKBJ7Ya_y/view?usp=sharing)

